# -*- coding: utf-8 -*-

"""
DeepLabV3 Semantic Segmentation Test - Pi Camera ile

Bu script, Raspberry Pi Camera Module v3 ile DeepLabV3 (Cityscapes)
modelini kullanarak gerçek zamanlı semantic segmentation yapar.

Model:
    models/deeplab_cityscapes.tflite

Girdi:
    513x513 RGB görüntü

Çıktı:
    Her piksel için sınıf tahmini (19 Cityscapes sınıfı)

Kullanım:
    python test_deeplabv3_picamera.py

Tuşlar:
    Q : Çıkış
    S : Ekran görüntüsü kaydet
    P : Segmentasyonu duraklat / devam ettir
"""

import cv2
import numpy as np
import time
import os
import threading


# ================= TENSORFLOW LITE =================
try:
    import tflite_runtime.interpreter as tflite
    TFLITE_RUNTIME = True
except ImportError:
    try:
        import tensorflow.lite as tflite
        TFLITE_RUNTIME = False
    except ImportError:
        print("HATA: TensorFlow Lite veya tflite_runtime yüklü değil!")
        print("Kurulum: pip install tflite-runtime")
        exit(1)


# ================= PI CAMERA =================
try:
    from picamera2 import Picamera2
    PICAMERA_AVAILABLE = True
except ImportError:
    PICAMERA_AVAILABLE = False
    print("UYARI: Picamera2 bulunamadı. USB kamera kullanılacak.")


# ================= CITYSCAPES SINIFLARI =================
CITYSCAPES_CLASSES = {
    0: ("road", "Yol", (128, 64, 128)),
    1: ("sidewalk", "Kaldırım", (244, 35, 232)),
    2: ("building", "Bina", (70, 70, 70)),
    3: ("wall", "Duvar", (102, 102, 156)),
    4: ("fence", "Çit", (190, 153, 153)),
    5: ("pole", "Direk", (153, 153, 153)),
    6: ("traffic_light", "Trafik Lambası", (250, 170, 30)),
    7: ("traffic_sign", "Trafik İşareti", (220, 220, 0)),
    8: ("vegetation", "Bitki Örtüsü", (107, 142, 35)),
    9: ("terrain", "Arazi", (152, 251, 152)),
    10: ("sky", "Gökyüzü", (70, 130, 180)),
    11: ("person", "İnsan", (220, 20, 60)),
    12: ("rider", "Sürücü", (255, 0, 0)),
    13: ("car", "Araba", (0, 0, 142)),
    14: ("truck", "Kamyon", (0, 0, 70)),
    15: ("bus", "Otobüs", (0, 60, 100)),
    16: ("train", "Tren", (0, 80, 100)),
    17: ("motorcycle", "Motosiklet", (0, 0, 230)),
    18: ("bicycle", "Bisiklet", (119, 11, 32)),
}


def create_color_palette():
    palette = np.zeros((256, 3), dtype=np.uint8)
    for cid, (_, _, color) in CITYSCAPES_CLASSES.items():
        palette[cid] = (color[2], color[1], color[0])  # BGR
    return palette


COLOR_PALETTE = create_color_palette()


# ================= PI CAMERA READER =================
class PiCameraReader:
    def __init__(self, camera_num=0, width=1280, height=720):
        if not PICAMERA_AVAILABLE:
            raise RuntimeError("Picamera2 kurulu değil!")

        self.running = True
        self.lock = threading.Lock()
        self.latest_frame = None

        self.picam2 = Picamera2(camera_num)
        config = self.picam2.create_preview_configuration(
            main={"size": (width, height), "format": "RGB888"},
            buffer_count=2
        )
        self.picam2.configure(config)
        self.picam2.start()
        time.sleep(1.0)

        self.thread = threading.Thread(target=self._update, daemon=True)
        self.thread.start()

        print(f"[OK] Pi Camera başlatıldı ({width}x{height})")

    def _update(self):
        while self.running:
            frame = self.picam2.capture_array()
            with self.lock:
                self.latest_frame = frame

    def read(self):
        with self.lock:
            if self.latest_frame is not None:
                return True, self.latest_frame.copy()
            return False, None

    def release(self):
        self.running = False
        self.picam2.stop()
        self.picam2.close()
        print("Pi Camera kapatıldı.")

    def isOpened(self):
        return self.running


# ================= USB CAMERA =================
class USBCameraReader:
    def __init__(self, src=0, width=1280, height=720):
        self.cap = cv2.VideoCapture(src)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)

        if not self.cap.isOpened():
            raise RuntimeError("USB Kamera açılamadı!")

        print("[OK] USB Kamera başlatıldı")

    def read(self):
        return self.cap.read()

    def release(self):
        self.cap.release()

    def isOpened(self):
        return self.cap.isOpened()


# ================= DEEPLAB SEGMENTER =================
class DeepLabSegmenter:
    def __init__(self, model_path):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model bulunamadı: {model_path}")

        self.interpreter = tflite.Interpreter(model_path=model_path)
        self.interpreter.allocate_tensors()

        self.input_details = self.interpreter.get_input_details()
        self.output_details = self.interpreter.get_output_details()

        self.h = self.input_details[0]['shape'][1]
        self.w = self.input_details[0]['shape'][2]
        self.dtype = self.input_details[0]['dtype']

        print("[OK] Model yüklendi")

    def preprocess(self, frame):
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        resized = cv2.resize(rgb, (self.w, self.h))

        if self.dtype == np.float32:
            resized = resized.astype(np.float32) / 255.0

        return np.expand_dims(resized, axis=0)

    def predict(self, frame):
        inp = self.preprocess(frame)
        self.interpreter.set_tensor(self.input_details[0]['index'], inp)
        self.interpreter.invoke()
        out = self.interpreter.get_tensor(self.output_details[0]['index'])

        if out.ndim == 4:
            mask = np.argmax(out[0], axis=-1)
        else:
            mask = out[0]

        return cv2.resize(
            mask.astype(np.uint8),
            (frame.shape[1], frame.shape[0]),
            interpolation=cv2.INTER_NEAREST
        )

    def visualize(self, frame, mask, alpha=0.5):
        colored = COLOR_PALETTE[mask]
        return cv2.addWeighted(frame, 1 - alpha, colored, alpha, 0)


# ================= MAIN =================
def main():
    model_path = os.path.join("models", "deeplab_cityscapes.tflite")

    segmenter = DeepLabSegmenter(model_path)

    camera = PiCameraReader() if PICAMERA_AVAILABLE else USBCameraReader()

    fps_counter = 0
    fps = 0
    t0 = time.time()

    while True:
        ret, frame = camera.read()
        if not ret:
            continue

        if PICAMERA_AVAILABLE:
            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

        start = time.time()
        mask = segmenter.predict(frame)
        infer_time = (time.time() - start) * 1000

        result = segmenter.visualize(frame, mask)

        fps_counter += 1
        if time.time() - t0 >= 1:
            fps = fps_counter
            fps_counter = 0
            t0 = time.time()

        cv2.putText(result, f"FPS: {fps}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
        cv2.putText(result, f"Inference: {infer_time:.1f} ms", (10, 65),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

        cv2.imshow("DeepLabV3 Cityscapes", result)

        if cv2.waitKey(1) & 0xFF in (ord('q'), ord('Q')):
            break

    camera.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
